只要提供适当的归属，谷歌特此授予允许在新闻或学术作品中独立使用本文中的表格和图表的权限。



Attention Is All You Need



Ashish Vaswani∗ Noam Shazeer∗ Niki Parmar∗ Jakob Uszkoreit∗

Google Brain Google Brain Google Research Google Research

avaswani@google.com noam@google.com nikip@google.com usz@google.com



Llion Jones∗ Aidan N. Gomez∗ † Łukasz Kaiser∗

Google Research University of Toronto Google Brain

llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com



Illia Polosukhin∗ ‡

illia.polosukhin@gmail.com



摘要



目前最主流的序列转换模型基于复杂的循环神经网络或卷积神经网络，包括一个编码器和一个解码器。性能最好的模型通过注意机制将编码器和解码器相连接。我们提出了一种新的简单的网络架构，Transformer，完全基于注意机制，不涉及循环和卷积。在两个机器翻译任务上的实验证明，这些模型在质量上优于其他模型，同时更易并行化，并且训练时间显著缩短。我们的模型在WMT 2014英-德翻译任务上实现了28.4的BLEU值，相较于现有最好的结果（包括集成模型）提高了2个BLEU。在WMT 2014英-法翻译任务中，我们的模型在经过3.5天的训练后，在仅使用八块GPU的情况下，达到了41.8的单模型BLEU得分，仅需文献中最佳模型训练成本的一小部分。我们还证明了Transformer在其他任务上具有良好的泛化能力，通过成功应用它在英语成分分析方面，无论是使用大规模还是有限的训练数据。



∗等同贡献。列表排序是随机的。Jakob提出用自注意力替代RNN并开始了对此思想的评估。Ashish与Illia共同设计并实现了第一个Transformer模型，并在此工作的各个方面起到了关键作用。Noam提出了扩展的点积注意力、多头注意力和无参数位置表示，并成为几乎参与每个细节的另一个人。Niki设计、实现、调优和评估了我们原始代码库和tensor2tensor中无数的模型变种。Llion还尝试了新颖的模型变种，负责我们的初始代码库，以及高效的推理和可视化工作。Lukasz和Aidan花了无数个漫长的日子设计tensor2tensor的各个部分，并替换了我们以前的代码库，大大改善了结果并加快了我们的研究进展。



† 在Google Brain期间完成的工作。

‡ 在Google Research期间完成的工作。

第31届神经信息处理系统会议（NIPS 2017），美国加利福尼亚州长滩。

---

1 简介



递归神经网络，长短期记忆[13]和门控递归[7]神经网络已被确定为序列建模和转导问题（如语言建模和机器翻译[35,2,5]）中的最先进方法。自那以后，许多努力一直在推动递归语言模型和编码器-解码器架构的界限[38,24,15]。



递归模型通常沿着输入和输出序列的符号位置进行计算。将位置与计算时间步齐，它们生成隐藏状态h的序列，作为前一个隐藏状态h和位置t的输入的函数。然而，这种顺序性的特性使得无法在训练示例中实现并行化，这在更长的序列长度中变得关键，因为内存限制限制了示例之间的批处理。最近的研究通过因式分解技巧[21]和条件计算[32]在计算效率方面取得了重大改进，同时还提高了模型性能。然而，顺序计算的根本限制仍然存在。



注意机制已成为引人注目的序列建模和转导模型的一个重要组成部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2,19]。然而，在除了少数几种情况[27]之外，这些注意机制通常与递归网络一起使用。



在这项工作中，我们提出了一种名为Transformer的模型架构，它不依赖于递归，而是完全依赖于注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更多的并行化，并且在经过8个P100GPU的训练仅需12小时后可以达到翻译质量的新水平。



2 背景知识

减少顺序计算的目标还是Extended NeuralGPU [16]，ByteNet [18]和ConvS2S [9]等的基础，它们使用卷积神经网络作为基本模块，在所有输入和输出位置上并行计算隐藏表示。在这些模型中，从两个任意输入或输出位置之间联系信号所需的操作数随着位置之间的距离的增加而增加，对于ConvS2S呈线性增长，对于ByteNet呈对数增长。这使得学习远距离位置之间的依赖关系更加困难。在Transformer中，这被减少为恒定数量的操作，尽管由于平均注意加权位置而导致有效分辨率降低，我们通过Multi-Head Attention在3.2节中描述的方法来抵消这种效果。



自注意力，有时也称为内部注意力，是一种注意机制，用于计算单个序列的不同位置之间的关系。自我注意力已成功应用于各种任务，包括阅读理解、摘要、文本蕴含和学习与任务无关的句子表示[4,27,28,22]。



端到端记忆网络是基于逐步注意机制而不是序列对齐递归的，已被证明在简单语言问答和语言建模任务中表现良好[34]。



据我们所知，Transformer是第一个完全依赖于自我注意力来计算其输入和输出表示的推导模型，而不使用序列对齐的RNN或卷积。在接下来的章节中，我们将描述Transformer，诠释自我注意力并讨论其相对于模型[17,18]和[9]的优势。



3 模型架构

大多数具有竞争力的神经序列转导模型都具有编码器-解码器结构[5,2,35]。在这里，编码器将输入符号表示的序列(x1，...，xn)映射到连续表示的序列z = (z1，...，zn)。给定z，解码器然后逐步生成一个输出符号的序列(y1，...，ym)。在每一步中，模型是自回归的[10]，即在生成下一个符号时，消耗先前生成的符号作为附加输入。

