在提供适当归属的情况下，谷歌特此授予权限，仅用于新闻或学术作品中使用本文中的表格和图表。



Attention Is All You Need



Ashish Vaswani∗ Noam Shazeer∗ Niki Parmar∗ Jakob Uszkoreit∗

Google Brain    Google Brain Google Research Google Research

avaswani@google.com noam@google.com nikip@google.com usz@google.com



Llion Jones∗   Aidan N. Gomez∗ †     Łukasz Kaiser∗

Google Research University of Toronto  Google Brain

llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com



Illia Polosukhin∗ ‡

illia.polosukhin@gmail.com



摘要



目前的主要序列转换模型基于复杂的循环或卷积神经网络，包括一个编码器和一个解码器。表现最好的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构，Transformer，它仅基于注意机制，完全放弃了循环和卷积。对两个机器翻译任务的实验证明，这些模型在质量上优于其他模型，同时更容易并行化，并且需要更少的训练时间。我们的模型在WMT 2014年英德翻译任务上达到了28.4 BLEU，相比现有最佳结果（包括集合）提升了2个BLEU。在WMT 2014年英法翻译任务中，我们的模型在训练8个GPU，3.5天后，达到了41.8 BLEU的新的单模型最优结果，训练成本只是文献中最佳模型的一小部分。我们还展示了Transformer在其他任务上具有广泛的泛化能力，通过成功应用它到大规模和有限训练数据的英语短语结构分析。



∗平等贡献。排列顺序是随机的。Jakob提出了用自注意力替换循环神经网络的想法，并开始评估这个想法。Ashish和Illia设计和实现了第一个Transformer模型，并在工作的各个方面发挥了关键作用。Noam提出了缩放的点积注意力、多头注意力和无参数位置表示，几乎涉及到了每一个细节。Niki在我们的原始代码库和Tensor2Tensor中设计、实现、调整和评估了无数的模型变体。Llion还尝试了新颖的模型变体，负责我们的初始代码库，以及高效的推断和可视化。Lukasz和Aidan花费了大量时间设计Tensor2Tensor的各个部分，并实现了替换我们早期代码库的工作，显著提高了结果并大大加速了我们的研究。



†在Google Brain期间执行的工作。

‡在Google Research期间执行的工作。

第31届神经信息处理系统会议(NIPS2017)，美国加利福尼亚州长滩。

---

1 引言



循环神经网络、长短时记忆网络[13]和门控循环神经网络[7]已经被牢固地确立为序列建模和转录问题（如语言建模和机器翻译）的最先进方法[35,2,5]。自那以后，许多努力一直在推动循环语言模型和编码器-解码器架构的界限[38,24,15]。

循环模型通常基于输入和输出序列中的符号位置进行计算。通过将位置与计算时间步骤对齐，它们生成作为位置t的前一个隐藏状态h和输入的函数的隐藏状态序列h。然而，由于其顺序性质，这种方法在训练示例中无法进行并行化，在更长的序列长度上变得关键，因为记忆限制限制了跨示例的批处理。最近的研究通过因式分解技巧[21]和条件计算[32]在计算效率上取得了显着的改进，同时在后者的情况下也提高了模型性能。然而，顺序计算的基本约束仍然存在。



在各种任务中，注意力机制已成为引人注目的序列建模和转导模型的一个重要组成部分，它允许依赖性的建模而不考虑它们在输入或输出序列中的距离[2,19]。然而除了少数几种情况[27]外，这种注意力机制通常与循环网络结合使用。



在这项工作中，我们提出了Transformer，这是一种架构模型，它摒弃了循环性，而完全依赖于注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更高程度的并行化，并在仅经过12小时在8台P100GPU上训练后达到了新的翻译质量水平。



2 背景



减少顺序计算的目标也是Extended Neural GPU[16]、ByteNet[18]和ConvS2S[9]的基础，它们都使用卷积神经网络作为基本构建块，为所有输入和输出位置同时计算隐藏表示。在这些模型中，从两个任意输入或输出位置相互关联所需的操作数量随位置之间的距离而增长，对于ConvS2S是线性增长，对于ByteNet是对数增长。这使得在远距离位置之间学习依赖关系变得更加困难。在Transformer中，这被减少为固定数量的操作，尽管由于对加权位置进行平均，导致有效分辨率降低，我们通过Multi-Head Attention来抵消这种效应，具体描述见第3.2节。



自注意力，有时被称为内部关注，是一种关注机制，用于计算序列的不同位置之间的表示。自注意力在包括阅读理解、抽象总结、文本蕴含和学习与任务无关的句子表示等多种任务中取得了成功[4,27,28,22]。



自始至终的记忆网络是基于一种循环关注机制而不是序列对齐重复，并已在简单语言问答和语言建模任务中表现良好[34]。



据我们所知，然而，Transformer是第一个完全依赖于自注意力计算其输入和输出的表示而不使用序列对齐的RNN或卷积的转导模型。在接下来的几节中，我们将描述Transformer，阐明自注意力的动机，并讨论它相对于模型[17,18]和[9]的优势。



3 模型架构



大多数有竞争力的神经序列转导模型都具有编码器-解码器结构[5,2,35]。在这里，编码器将符号表示的输入序列(x1,...,xn)映射到连续表示的序列z1,...,zn。给定z，解码器然后生成一个输出序列(y1,...,ym)的符号，每次一次生成一个元素。在每个步骤中，模型是自回归的[10]，在生成下一个元素时将先前生成的符号作为额外的输入。

